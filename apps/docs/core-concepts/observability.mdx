---
title: "Observability & Logging"
description: "Monitor and debug your AI agents with built-in observability features, including new metrics for the Gemini model"
---

SpinAI provides built-in observability features to help you monitor and debug your AI agents. By enabling logging, you can track agent interactions, performance metrics, and costs in real-time through the SpinAI dashboard.

## Getting Started

To enable observability for your agents:

1. Sign in to [app.spinai.dev](https://app.spinai.dev)
2. Create an organization
3. Generate a SpinAI API key

Then, configure your agent with the API key and a unique agent ID:

```typescript
const agent = createAgent({
  instructions: "You are a customer support agent.",
  actions: [getCustomerInfo, getSubscriptionStatus, createTicket],
  llm,
  // Enable observability with these two fields:
  agentId: "customer-support-agent", // Choose any unique identifier
  spinApiKey: process.env.SPINAI_API_KEY,
});
```

## What's Being Tracked

When observability is enabled, SpinAI automatically tracks:

- Agent interactions and their outcomes
- Model usage and token consumption
- Cost metrics per interaction
- Response times and latency
- Evaluation steps and reasoning
- Action executions and their results
- Errors and failure cases

## Viewing Logs

Visit [app.spinai.dev](https://app.spinai.dev) to view your agent logs and metrics. The dashboard provides:

- Real-time monitoring of agent activities
- Detailed interaction histories
- Cost and performance analytics
- Error tracking and debugging tools

All interactions are grouped by agent ID, making it easy to monitor specific agents or use cases within your application.

## New Observability Features for Gemini

The introduction of the Gemini model brings new observability features to SpinAI. With Gemini, you gain access to enhanced metrics that provide deeper insights into the performance and cost-effectiveness of your AI agents.

### Enhanced Metrics

- **Input Tokens:** The number of tokens used in the input prompt to the Gemini model.
- **Output Tokens:** The number of tokens generated by the Gemini model in response to the input.
- **Cost Estimation:** An estimate of the cost for each interaction with the Gemini model, calculated based on the number of input and output tokens.

These metrics are automatically tracked for every interaction with the Gemini model, allowing you to monitor usage and optimize costs more effectively.

### Usage Example

When using the Gemini model, observability metrics can be accessed in the completion result:

```typescript
import { createGeminiLLM } from '@your-org/spinai';

const geminiLLM = createGeminiLLM({
  apiKey: process.env.GEMINI_API_KEY,
  model: 'gemini-2.0-flash',
});

const result = await geminiLLM.complete({
  prompt: "Explain the concept of observability in AI systems.",
  maxTokens: 250,
});

console.log(`Input Tokens: ${result.inputTokens}`);
console.log(`Output Tokens: ${result.outputTokens}`);
console.log(`Estimated Cost: ${result.costCents} cents`);
```

This example demonstrates how to access the new observability metrics provided by the Gemini model, enabling you to gain insights into token usage and interaction costs.

By leveraging these new features, you can ensure that your AI agents are not only effective but also cost-efficient.