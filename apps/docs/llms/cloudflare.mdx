---
title: "Cloudflare LLM Integration"
description: "Integrating Cloudflare's LLM for natural language processing with SpinAI."
---

## Introduction

The Cloudflare LLM integration allows you to leverage Cloudflare's AI models for natural language processing within the SpinAI framework. This document outlines how to configure the Cloudflare LLM and utilize its completion functions within your applications.

## Configuration

To use the Cloudflare LLM, you must first configure it with your Cloudflare account details. The configuration requires an API token and an Account ID. Optionally, you can specify a model to use; if not provided, a default model is used.

```typescript
interface CloudflareConfig {
  apiToken: string;
  accountId: string;
  model?: string; // Optional, defaults to "@cf/meta/llama-2-7b-chat-int8"
}
```

### Basic Configuration Example

```typescript
import { createCloudflareAILLM } from "spinai";

const llm = createCloudflareAILLM({
  apiToken: process.env.CLOUDFLARE_API_TOKEN,
  accountId: process.env.CLOUDFLARE_ACCOUNT_ID,
  model: "@cf/meta/llama-2-7b-chat-int8", // Optional
});
```

## Completion Functions

The completion function allows you to send prompts to the Cloudflare LLM and receive generated text based on the model's training. You can specify options such as the prompt, schema for the expected response, temperature, and maximum tokens.

```typescript
async complete<T>({
  prompt: string,
  schema?: object,
  temperature?: number,
  maxTokens?: number,
}): Promise<CompletionResult<T>>
```

### Completion Function Usage

```typescript
const completionOptions = {
  prompt: "Explain the theory of relativity",
  temperature: 0.7,
  maxTokens: 1024,
};

llm.complete(completionOptions)
  .then(response => console.log(response.content))
  .catch(error => console.error(error));
```

## Token Cost Calculations

The Cloudflare LLM integration includes a utility to estimate the cost of each completion request. Cloudflare AI does not provide token counts directly, so the cost is estimated based on the character count of the input and output. This estimation helps in managing and forecasting usage costs.

```typescript
const inputChars = prompt.length;
const outputChars = JSON.stringify(content).length;
const estimatedInputTokens = Math.ceil(inputChars / 4);
const estimatedOutputTokens = Math.ceil(outputChars / 4);

const costCents = calculateCost(estimatedInputTokens, estimatedOutputTokens, model);
```

This document provides a foundational overview for integrating and utilizing Cloudflare's LLM with SpinAI. For further details on managing configurations, handling completion responses, and estimating costs, refer to the respective sections above.